{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-TiT8tGu0NY"
      },
      "outputs": [],
      "source": [
        "#install dependencies/libraries\n",
        "!pip install numpy==1.23.5\n",
        "!pip install pandas==1.5.3\n",
        "!pip install scikit-learn==1.2.1\n",
        "!pip install tensorflow==2.12.0\n",
        "!pip install keras==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWCCgF5y03fL",
        "outputId": "76cf68b4-3678-43ae-d63c-f738c2ddc628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W8bYDgFy9cc6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Flatten, Reshape, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3XOCk4Ec_zSn"
      },
      "outputs": [],
      "source": [
        "# Read data from Excel CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/transformed_data.csv')\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "# Prepare features and target\n",
        "ftr = df.iloc[:,:-1] # Excludes target feature\n",
        "target = df['DeepFake']  # Target Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bUfAMHew_4Pt"
      },
      "outputs": [],
      "source": [
        "# Splitting dataset to training and testing\n",
        "ftr_train, ftr_test, target_train, target_test = train_test_split(ftr, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RhsArvAad5dY"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(input_shape,)),\n",
        "        Dense(512),\n",
        "        LeakyReLU(),\n",
        "        Dropout(0.3),\n",
        "        Dense(256),\n",
        "        LeakyReLU(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(input_shape,)),\n",
        "        Dense(512, kernel_regularizer=regularizers.l2(0.01)),\n",
        "        LeakyReLU(),\n",
        "        Dropout(0.3),\n",
        "        Dense(256, kernel_regularizer=regularizers.l2(0.01)),\n",
        "        LeakyReLU(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, kernel_regularizer=regularizers.l2(0.01)),\n",
        "        LeakyReLU(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "D4nP7MuM-f8Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vkkiY5DHeBjg"
      },
      "outputs": [],
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Dense(512))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Dense(np.product([190]), activation='tanh'))\n",
        "    model.add(layers.Reshape((190,)))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGiDirEzeEsZ",
        "outputId": "48e4c96f-cd98-4532-bbef-72ddb3222ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-fb41223a8d5f>:2: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
            "  generator = make_generator_model()\n"
          ]
        }
      ],
      "source": [
        "# Initialize models with correct input shapes\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model(ftr_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "naWzo8gdttmW"
      },
      "outputs": [],
      "source": [
        "# Define the loss and optimizers\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eMtNk72peGRk"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "@tf.function\n",
        "def train_step(images, batch_size):\n",
        "    noise = tf.random.normal([batch_size, 100])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OK2iurYzeIU4"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for image_batch in dataset:\n",
        "            train_step(image_batch[0], tf.shape(image_batch[0])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6IvgZKBMxFfU"
      },
      "outputs": [],
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3b3OUwyTxIYi"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.zeros_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7joG1OEZt3a9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((ftr_train.values, target_train)).batch(BATCH_SIZE)\n",
        "train(train_dataset, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1EmbCj1Lt46E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40eb5f1-2c86-45b3-ff5c-fc8c03b4ce42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# Save generator\n",
        "generator.save('/content/drive/MyDrive/Colab Notebooks/GAN_ML/Models/GAN_generator_model_extended_ftr_full.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7FfYOgenGotZ"
      },
      "outputs": [],
      "source": [
        "# Save discriminator\n",
        "discriminator.save('/content/drive/MyDrive/Colab Notebooks/GAN_ML/Models/GAN_discriminator_model_extended_ftr_full.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmcM30_wNbei"
      },
      "outputs": [],
      "source": [
        "# -- optional -- load pretrained generator and discriminator.\n",
        "from keras.models import load_model\n",
        "discriminator = load_model('/content/drive/MyDrive/Colab Notebooks/GAN_ML/Models/binary_discriminator_model_model_extended_ftr_full.h5')\n",
        "generator = load_model('/content/drive/MyDrive/Colab Notebooks/GAN_ML/Models/binary_generator_model_extended_ftr_full.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VJgLotPUueN0"
      },
      "outputs": [],
      "source": [
        "# Function to generate fake images for testing\n",
        "def generate_fake_images(generator, num_images):\n",
        "    noise = tf.random.normal([num_images, 100])\n",
        "    generated_images = generator(noise, training=False)\n",
        "    return generated_images.numpy()  # Convert tensors to numpy arrays for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rDCglVMRuD5s"
      },
      "outputs": [],
      "source": [
        "def evaluate_discriminator(model, real_images, fake_images):\n",
        "    # Concatenate real and fake images into one array\n",
        "    images = np.concatenate([real_images, fake_images])\n",
        "    # Correct label assignment: 1s for fake, 1s for real\n",
        "    labels = np.concatenate([np.zeros(len(real_images)), np.ones(len(fake_images))])\n",
        "\n",
        "    # Shuffle the data to mix real and fake images\n",
        "    indices = np.arange(len(images))\n",
        "    np.random.shuffle(indices)\n",
        "    images = images[indices]\n",
        "    labels = labels[indices]\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(images, labels, verbose=1)  # verbosity for detailed output\n",
        "    print(f\"Loss: {loss}, Accuracy: {accuracy * 100}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FPWlbGituG3u"
      },
      "outputs": [],
      "source": [
        "# Preparing real images from ftr_test_final for testing\n",
        "real_images_test = ftr_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1am63hsGuNCV"
      },
      "outputs": [],
      "source": [
        "# Generate fake images for testing\n",
        "num_test_images = real_images_test.shape[0]  # Generate as many fake images as there are real test images\n",
        "fake_images_test = generate_fake_images(generator, num_test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JGOX6lGw7wQN"
      },
      "outputs": [],
      "source": [
        "# Compile the discriminator model\n",
        "discriminator.compile(\n",
        "    optimizer=discriminator_optimizer,  # You can adjust the learning rate and other parameters as needed\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVXIluu7uOeF",
        "outputId": "04c42af8-8ab4-49e9-8b0f-31f8b55aabf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "709/709 [==============================] - 2s 3ms/step - loss: 80.4562 - accuracy: 1.0000\n",
            "Loss: 80.4561767578125, Accuracy: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the discriminator\n",
        "evaluate_discriminator(discriminator, real_images_test, fake_images_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
